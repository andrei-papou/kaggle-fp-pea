{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport math\nimport os\nimport pytz\nimport random\nimport time\nimport warnings\nfrom tqdm.autonotebook import tqdm\nimport codecs\nfrom typing import Dict, List, Tuple\n\nimport pandas as pd\nimport numpy as np\nimport wandb\nfrom scipy import stats\nfrom nltk import ngrams\nfrom text_unidecode import unidecode\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\n\nimport torch\nimport transformers\n\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.nn.functional import one_hot\nfrom torch.optim import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset,WeightedRandomSampler\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, log_loss\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\n\n%env TOKENIZERS_PARALLELISM=true\nwarnings.filterwarnings(\"ignore\")\n\nprint(torch.cuda.get_device_name() if torch.cuda.is_available() else 'No Cuda')\nprint(f'torch.__version__: {torch.__version__}')\nprint(f'transformers.__version__: {transformers.__version__}')","metadata":{"id":"LBifslPJRnAX","outputId":"7ea9e477-d821-4680-903e-de4af51cdd77","execution":{"iopub.status.busy":"2022-08-11T13:32:58.154956Z","iopub.execute_input":"2022-08-11T13:32:58.155662Z","iopub.status.idle":"2022-08-11T13:33:06.015172Z","shell.execute_reply.started":"2022-08-11T13:32:58.155568Z","shell.execute_reply":"2022-08-11T13:33:06.013926Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"wandb.login(key='f1dfc080cc5d63892dc49e00fa50b664e85706f8')\nrun = wandb.init(project='LSTM',\n                  name='exp_9',\n                  save_code=True)","metadata":{"id":"dNN7TkgBSXFE","outputId":"212e20e0-fb26-4d83-cbba-485eeacf7335","execution":{"iopub.status.busy":"2022-08-11T13:33:06.017363Z","iopub.execute_input":"2022-08-11T13:33:06.018038Z","iopub.status.idle":"2022-08-11T13:33:11.446604Z","shell.execute_reply.started":"2022-08-11T13:33:06.017998Z","shell.execute_reply":"2022-08-11T13:33:11.445497Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    \n    f_x = np.exp(x) / np.sum(np.exp(x))\n    return f_x","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:33:11.448187Z","iopub.execute_input":"2022-08-11T13:33:11.448746Z","iopub.status.idle":"2022-08-11T13:33:12.336677Z","shell.execute_reply.started":"2022-08-11T13:33:11.448705Z","shell.execute_reply":"2022-08-11T13:33:12.335553Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"f0 = pd.read_csv('../input/preds-by-v5dbls42/prediction0.csv')\nf1 = pd.read_csv('../input/preds-by-v5dbls42/prediction1.csv')\nf2 = pd.read_csv('../input/preds-by-v5dbls42/prediction2.csv')\nf3 = pd.read_csv('../input/preds-by-v5dbls42/prediction3.csv')\nf4 = pd.read_csv('../input/preds-by-v5dbls42/prediction4.csv')\npreds = pd.read_csv('../input/predict-test-set/predicted_test_by_v5DBlS42.csv')","metadata":{"id":"o_SG7gwMRnAb","execution":{"iopub.status.busy":"2022-08-11T13:33:12.340163Z","iopub.execute_input":"2022-08-11T13:33:12.340849Z","iopub.status.idle":"2022-08-11T13:33:15.904278Z","shell.execute_reply.started":"2022-08-11T13:33:12.340812Z","shell.execute_reply":"2022-08-11T13:33:15.903338Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/for-lstm2/TRAIN_CATBOOST.csv')\ndf_test = pd.read_csv('../input/for-lstm2/TEST_CATBOOST.csv')","metadata":{"id":"Z7lKPHXyRnAc","execution":{"iopub.status.busy":"2022-08-11T13:33:15.905701Z","iopub.execute_input":"2022-08-11T13:33:15.906057Z","iopub.status.idle":"2022-08-11T13:33:16.876431Z","shell.execute_reply.started":"2022-08-11T13:33:15.906020Z","shell.execute_reply":"2022-08-11T13:33:16.875501Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.merge(preds, left_on='discourse_id', right_on='discourse_id')","metadata":{"id":"fQI6nyiaRnAf","execution":{"iopub.status.busy":"2022-08-11T13:33:16.877948Z","iopub.execute_input":"2022-08-11T13:33:16.878321Z","iopub.status.idle":"2022-08-11T13:33:17.774191Z","shell.execute_reply.started":"2022-08-11T13:33:16.878286Z","shell.execute_reply":"2022-08-11T13:33:17.773305Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"f_full = pd.concat([f0, f1, f2, f3, f4])[['discourse_id', 'Ineffective', 'Adequate', 'Effective']]\nf_full[['Ineffective', 'Adequate', 'Effective']] = f_full[['Ineffective', 'Adequate', 'Effective']].apply(softmax, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:33:17.775720Z","iopub.execute_input":"2022-08-11T13:33:17.776096Z","iopub.status.idle":"2022-08-11T13:33:35.063444Z","shell.execute_reply.started":"2022-08-11T13:33:17.776060Z","shell.execute_reply":"2022-08-11T13:33:35.062282Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(f_full, left_on='discourse_id', right_on='discourse_id')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:33:35.064827Z","iopub.execute_input":"2022-08-11T13:33:35.065169Z","iopub.status.idle":"2022-08-11T13:33:35.863385Z","shell.execute_reply.started":"2022-08-11T13:33:35.065133Z","shell.execute_reply":"2022-08-11T13:33:35.860486Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    accomulate = 4\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    lr = 1.e-4\n    epochs = 100","metadata":{"id":"1Zx7-QfuRnAf","execution":{"iopub.status.busy":"2022-08-11T13:33:35.864607Z","iopub.execute_input":"2022-08-11T13:33:35.865162Z","iopub.status.idle":"2022-08-11T13:33:36.969127Z","shell.execute_reply.started":"2022-08-11T13:33:35.865116Z","shell.execute_reply":"2022-08-11T13:33:36.968168Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DatasetLSTM(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.essay_id = self.df.essay_id.unique()\n    \n    def __len__(self):\n        return len(self.essay_id)\n    \n    def __getitem__(self, item):\n        span_df = self.df[self.df.essay_id == self.essay_id[item]].sort_values(by='start_pos')\n        disc_id = span_df.discourse_id.tolist()\n        inputs = span_df.loc[:, ['code_topic', 'code_dis_type', 'start_pos', 'end_pos', 'len_text',\n                   'len_essay', 'size_essay', 'Ineffective', 'Adequate', 'Effective']].values\n        label = np.zeros((len(span_df), 3))\n        label[np.arange(len(span_df)), span_df.label.tolist()] = 1.0\n\n        return disc_id, {\n                        'inputs': torch.tensor(inputs, dtype=torch.float),\n                        'label':  torch.tensor(label, dtype=torch.float)\n                        }","metadata":{"id":"e1Jv4ADgRnAg","execution":{"iopub.status.busy":"2022-08-11T13:33:37.833369Z","iopub.execute_input":"2022-08-11T13:33:37.833708Z","iopub.status.idle":"2022-08-11T13:33:39.751925Z","shell.execute_reply.started":"2022-08-11T13:33:37.833673Z","shell.execute_reply":"2022-08-11T13:33:39.750997Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n","metadata":{"id":"LUjUpJgkRnAg","execution":{"iopub.status.busy":"2022-08-11T13:33:39.753252Z","iopub.execute_input":"2022-08-11T13:33:39.754505Z","iopub.status.idle":"2022-08-11T13:33:40.727307Z","shell.execute_reply.started":"2022-08-11T13:33:39.754465Z","shell.execute_reply":"2022-08-11T13:33:40.726344Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def crit(y_preds, y_true):\n    return (-y_true*y_preds.log()).sum(dim=1).mean()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:33:40.728566Z","iopub.execute_input":"2022-08-11T13:33:40.728898Z","iopub.status.idle":"2022-08-11T13:33:41.686715Z","shell.execute_reply.started":"2022-08-11T13:33:40.728863Z","shell.execute_reply":"2022-08-11T13:33:41.685410Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def val_step(model, data_loader, cfg=CFG):\n    model.eval()\n    loss_stat = AverageMeter()\n    _out = []\n    _lab = []\n    for step, (disc_id, inputs) in enumerate(data_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(cfg.device)\n        with torch.no_grad():\n            out = model(inputs['inputs'])\n        _out.append(out.to('cpu'))\n        _lab.append(inputs['label'][0].to('cpu'))\n    _out = torch.concat(_out).softmax(dim=1).numpy()\n    _lab = torch.concat(_lab).numpy()\n    return log_loss(_lab, _out)\n        ","metadata":{"id":"eUtajEMFRnAj","execution":{"iopub.status.busy":"2022-08-11T13:33:41.687926Z","iopub.execute_input":"2022-08-11T13:33:41.688310Z","iopub.status.idle":"2022-08-11T13:33:42.457636Z","shell.execute_reply.started":"2022-08-11T13:33:41.688268Z","shell.execute_reply":"2022-08-11T13:33:42.456407Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, train, test, cfg):\n    best_loss = float('inf')\n    train_dataset = DatasetLSTM(train)\n    test_dataset = DatasetLSTM(test)\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    it = tqdm(range(cfg.epochs), desc='Training.',  total=len(range(cfg.epochs)))\n    param_lrs = [{'params': param, 'lr': cfg.lr} for param in model.parameters()]\n    optimizer = torch.optim.AdamW(param_lrs, lr=cfg.lr)\n    scheduler =  get_cosine_schedule_with_warmup(optimizer=optimizer,\n                                               num_warmup_steps=0,\n                                               num_training_steps=len(train_loader)*cfg.epochs,\n                                               num_cycles=0.5)\n    \n\n    for epoch in it:\n        loss_stat = AverageMeter()\n        model.train()\n        for step, (disc_id, inputs) in enumerate(train_loader):\n            for k, v in inputs.items():\n                inputs[k] = v.to(cfg.device)\n            out = model(inputs['inputs'])\n#             print(out)\n#             print(inputs['label'][0])\n            loss = loss_func(out, inputs['label'][0])\n            loss_stat.update(loss, 1)\n            loss = loss / cfg.accomulate\n            loss.backward()\n            if (step+1) % cfg.accomulate == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                scheduler.step()\n            it.set_description(\n                    f'Training | '\n                    f'epoch: {epoch} | '\n                    f'loss: {loss_stat.avg:.5f} | ' )\n        avg_val_loss = val_step(model, test_loader, cfg)\n        model.train()\n        wandb.log({'val_loss': avg_val_loss})\n        if best_loss > avg_val_loss:\n            print(f'impruved best_loss form {best_loss} to {avg_val_loss}')\n            best_loss = avg_val_loss\n            path = OUTPUT + 'LSTM_final'\n            torch.save({'model': model.state_dict()}, path+'.pth')","metadata":{"id":"zBYTw8GiRnAj","execution":{"iopub.status.busy":"2022-08-11T13:35:58.458436Z","iopub.execute_input":"2022-08-11T13:35:58.458798Z","iopub.status.idle":"2022-08-11T13:35:59.565419Z","shell.execute_reply.started":"2022-08-11T13:35:58.458766Z","shell.execute_reply":"2022-08-11T13:35:59.564405Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss(reduction='mean')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:35:59.567373Z","iopub.execute_input":"2022-08-11T13:35:59.567714Z","iopub.status.idle":"2022-08-11T13:36:00.327932Z","shell.execute_reply.started":"2022-08-11T13:35:59.567679Z","shell.execute_reply":"2022-08-11T13:36:00.327012Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(11, 3, num_layers=3)\n#         self.seq = nn.Sequential(nn.Linear(64, 16),\n#                                  nn.LayerNorm(16),\n#                                  nn.Linear(16, 3))\n    \n    def forward(self, inputs):\n        result = self.lstm(inputs)[0]\n        #res = self.seq(result)\n        return result[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-11T13:36:00.329588Z","iopub.execute_input":"2022-08-11T13:36:00.329936Z","iopub.status.idle":"2022-08-11T13:36:01.240017Z","shell.execute_reply.started":"2022-08-11T13:36:00.329900Z","shell.execute_reply":"2022-08-11T13:36:01.239035Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    OUTPUT = './'\n    model = Model()\n    model.to(CFG.device)\n    train_loop(model, df_train, df_test, CFG)","metadata":{"id":"2XjXgxtyRnAj","outputId":"2aa7cfe4-2bdb-429f-8795-052991afcd71","execution":{"iopub.status.busy":"2022-08-11T13:36:01.241989Z","iopub.execute_input":"2022-08-11T13:36:01.242380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"CNyCaocgRnAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}