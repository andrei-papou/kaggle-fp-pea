{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import codecs\n",
    "import itertools\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as torch_f\n",
    "import typing as t\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from text_unidecode import unidecode\n",
    "from torch.utils.data import Dataset as TorchDataset, default_collate as default_collate_fn, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import set_seed as set_huggingface_seed\n",
    "from transformers.utils.generic import PaddingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    np.random.seed(seed % (2 ** 32 - 1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "    set_huggingface_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe(x: t.Any) -> pd.DataFrame:\n",
    "    return t.cast(pd.DataFrame, x)\n",
    "\n",
    "\n",
    "def series(x: t.Any) -> pd.Series:\n",
    "    return t.cast(pd.Series, x)\n",
    "\n",
    "\n",
    "_T = t.TypeVar('_T')\n",
    "\n",
    "\n",
    "def unwrap_opt(x: t.Optional[_T]) -> _T:\n",
    "    assert x is not None\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_csv(path: Path) -> pd.DataFrame:\n",
    "    return dataframe(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValStrategy:\n",
    "\n",
    "    def _copy_with_fold(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['fold'] = -1\n",
    "        return df\n",
    "\n",
    "    def assign_folds(self, df: pd.DataFrame, num_folds: int) -> pd.DataFrame:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class StratifyAndGroupByColumnValStrategy(ValStrategy):\n",
    "\n",
    "    def __init__(self, stratify_by: str, group_by: str, seed: int):\n",
    "        self._stratify_by = stratify_by\n",
    "        self._group_by = group_by\n",
    "        self._seed = seed\n",
    "\n",
    "    def assign_folds(self, df: pd.DataFrame, num_folds: int) -> pd.DataFrame:\n",
    "        df = self._copy_with_fold(df)\n",
    "        stratify_series = series(df[self._stratify_by])\n",
    "        group_series = series(df[self._group_by])\n",
    "        kf = StratifiedGroupKFold(n_splits=num_folds, shuffle=True, random_state=self._seed)\n",
    "        for f, (t_, v_) in enumerate(kf.split(X=df, y=stratify_series, groups=group_series.values)):\n",
    "            df.loc[v_, 'fold'] = f\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISC_TYPE_TO_TOK = {\n",
    "    'Lead': 'LD',\n",
    "    'Position': 'PS',\n",
    "    'Claim': 'CL',\n",
    "    'Evidence': 'EV',\n",
    "    'Counterclaim': 'CCL',\n",
    "    'Rebuttal': 'RB',\n",
    "    'Concluding Statement': 'CS',\n",
    "}\n",
    "TOK_PHR = 'PH'\n",
    "\n",
    "def parse_special_token_list(df: pd.DataFrame) -> t.List[str]:\n",
    "    return [\n",
    "        # f'[{str(tok).replace(\" \", \"-\").upper()}]' for tok in list(df['discourse_type'].unique())\n",
    "        # TOK_PHR,\n",
    "        *list(DISC_TYPE_TO_TOK.values()),\n",
    "    ]\n",
    "\n",
    "\n",
    "def join_special_token_lists(*special_token_lists: t.List[str]) -> t.List[str]:\n",
    "    result = []\n",
    "    for stl in special_token_lists:\n",
    "        for tok in stl:\n",
    "            if tok not in result:\n",
    "                result.append(tok)\n",
    "    return result\n",
    "\n",
    "\n",
    "ModelInputValue = t.Union[torch.Tensor, bool]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizerResult:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "    def get_model_input(self, device) -> t.Dict[str, ModelInputValue]:\n",
    "        return {\n",
    "            'input_ids': self.input_ids.to(device),\n",
    "            'attention_mask': self.attention_mask.to(device),\n",
    "        }\n",
    "\n",
    "    def to_collatable_dict(self) -> t.Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            'input_ids': self.input_ids,\n",
    "            'attention_mask': self.attention_mask,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_collateable_dict(cls, val: t.Dict[str, torch.Tensor]) -> TokenizerResult:\n",
    "        return cls(**val)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    result_type: t.Type[TokenizerResult]\n",
    "\n",
    "    def __init__(self, padding_strategy: PaddingStrategy = PaddingStrategy.MAX_LENGTH):\n",
    "        self._padding_strategy = padding_strategy\n",
    "        self._special_token_list = []\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self) -> PreTrainedTokenizerBase:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    @property\n",
    "    def cls_token(self) -> str:\n",
    "        return self.tokenizer.cls_token\n",
    "\n",
    "    @property\n",
    "    def sep_token(self) -> str:\n",
    "        return self.tokenizer.sep_token\n",
    "\n",
    "    @property\n",
    "    def num_special_tokens(self) -> int:\n",
    "        return len(self._special_token_list)\n",
    "\n",
    "    def add_special_token_list(self, tok_list: t.List[str]):\n",
    "        self._special_token_list.extend(tok_list)\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': tok_list})\n",
    "\n",
    "    def _build_result(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            token_type_ids: torch.Tensor,) -> TokenizerResult:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tokenize(\n",
    "            self,\n",
    "            *texts: str,\n",
    "            max_len: int) -> TokenizerResult:\n",
    "        encoding = self.tokenizer(\n",
    "            *texts,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding=self._padding_strategy,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True)  # type: ignore\n",
    "        input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(encoding['token_type_ids'], dtype=torch.long)\n",
    "\n",
    "        return self._build_result(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DebertaTokenizerResult(TokenizerResult):\n",
    "    token_type_ids: torch.Tensor\n",
    "\n",
    "    def get_model_input(self, device: str) -> t.Dict[str, ModelInputValue]:\n",
    "        return {\n",
    "            **super().get_model_input(device=device),\n",
    "            'token_type_ids': self.token_type_ids.to(device=device),\n",
    "        }\n",
    "\n",
    "    def to_collatable_dict(self) -> t.Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            **super().to_collatable_dict(),\n",
    "            'token_type_ids': self.token_type_ids,\n",
    "        }\n",
    "\n",
    "\n",
    "class DebertaTokenizer(Tokenizer):\n",
    "    result_type = DebertaTokenizerResult\n",
    "\n",
    "    def __init__(self, checkpoint: str, padding_strategy: PaddingStrategy = PaddingStrategy.MAX_LENGTH):\n",
    "        super().__init__(padding_strategy=padding_strategy)\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self) -> PreTrainedTokenizerBase:\n",
    "        return self._tokenizer\n",
    "\n",
    "    def _build_result(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            token_type_ids: torch.Tensor) -> TokenizerResult:\n",
    "        return DebertaTokenizerResult(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BACKBONE_TO_TOKENIZER_TYPE = {\n",
    "    'microsoft/deberta-v3-small': DebertaTokenizer,\n",
    "    'microsoft/deberta-v3-base': DebertaTokenizer,\n",
    "    'microsoft/deberta-v3-large': DebertaTokenizer,\n",
    "}\n",
    "\n",
    "def get_tokenizer_for_backbone(\n",
    "        backbone: str,\n",
    "        checkpoint: str | None = None,\n",
    "        padding_strategy: PaddingStrategy = PaddingStrategy.MAX_LENGTH) -> Tokenizer:\n",
    "    checkpoint = checkpoint if checkpoint is not None else backbone\n",
    "    tokenizer_type = _BACKBONE_TO_TOKENIZER_TYPE.get(backbone)\n",
    "    if tokenizer_type is None:\n",
    "        raise ValueError(f'Backbone \"{backbone}\" is not supported.')\n",
    "    return tokenizer_type(checkpoint, padding_strategy=padding_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_encoding_with_utf8(error: UnicodeError) -> t.Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> t.Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetItem:\n",
    "    id: t.List[str]\n",
    "    tokenizer_result: TokenizerResult\n",
    "\n",
    "\n",
    "TorchCollator = t.Callable[[t.List[t.Dict[str, torch.Tensor]]], t.Dict[str, torch.Tensor]]\n",
    "\n",
    "\n",
    "class Collator:\n",
    "\n",
    "    def __init__(self, tokenizer_result_collator: TorchCollator = default_collate_fn):\n",
    "        self._tokenizer_result_collator = tokenizer_result_collator\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            item_list: t.List[DatasetItem]) -> DatasetItem:\n",
    "        assert len(item_list) > 0\n",
    "        tokenizer_result_type = type(item_list[0].tokenizer_result)\n",
    "        return DatasetItem(\n",
    "            id=sum([item.id for item in item_list], []),\n",
    "            tokenizer_result=tokenizer_result_type.from_collateable_dict(\n",
    "                self._tokenizer_result_collator([item.tokenizer_result.to_collatable_dict() for item in item_list])))\n",
    "\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    _CLS_TO_INT_DICT = {\n",
    "        'Ineffective': 0,\n",
    "        'Adequate': 1,\n",
    "        'Effective': 2,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: Tokenizer,\n",
    "            max_len: int,):\n",
    "        self._df = df.copy().reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def _get_tokenizer_input(self, row: t.Dict[str, t.Any]) -> str:\n",
    "        (\n",
    "            disc_type,\n",
    "            text,\n",
    "            essay_id,\n",
    "            essay_text,\n",
    "            pos,\n",
    "         ) = (\n",
    "            str(row['discourse_type']),\n",
    "            str(row['discourse_text']),\n",
    "            str(row['essay_id']),\n",
    "            str(row['essay_text']),\n",
    "            int(row['pos']),\n",
    "         )\n",
    "        sep = self._tokenizer.sep_token\n",
    "        max_pos = max([\n",
    "            int(row['pos'])\n",
    "            for _, row in self._df[self._df['essay_id'] == essay_id].sort_values('pos').iterrows()\n",
    "        ])\n",
    "\n",
    "        tokenizer_input = f'{disc_type} {pos} / {max_pos} {sep} {text} {sep} {essay_text}'\n",
    "\n",
    "        return tokenizer_input\n",
    "\n",
    "    def sort_by_tokenizer_input_len(self):\n",
    "        self._df['_tok_input_len'] = self._df.progress_apply(self._get_tokenizer_input, axis=1)\n",
    "        self._df = self._df.sort_values('_tok_input_len')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> DatasetItem:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        tokenizer_input = self._get_tokenizer_input(row)\n",
    "\n",
    "        (\n",
    "            id,\n",
    "        ) = (\n",
    "            str(row['discourse_id']),\n",
    "        )\n",
    "\n",
    "        tokenizer_result = self._tokenizer.tokenize(\n",
    "            tokenizer_input, max_len=self._max_len)\n",
    "\n",
    "        return DatasetItem(\n",
    "            id=[id],\n",
    "            tokenizer_result=tokenizer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_k_fold_distribution(df: pd.DataFrame, stratify_by: str) -> pd.DataFrame:\n",
    "    cls_list = sorted(df[stratify_by].unique())\n",
    "    row_list = []\n",
    "    for fold in sorted(series(df['fold']).unique()):\n",
    "        fold_df = dataframe(df[df['fold'] == fold])\n",
    "        row_list.append({\n",
    "            'fold': fold,\n",
    "            'num_samples': len(fold_df),\n",
    "            'mean_discourse_text_len': fold_df['discourse_text_len'].mean(),\n",
    "            'std_discourse_text_len': fold_df['discourse_text_len'].std(),\n",
    "            'mean_essay_text_len': fold_df['essay_text_len'].mean(),\n",
    "            'std_essay_text_len': fold_df['essay_text_len'].std(),\n",
    "            **{\n",
    "                f'num_{cls.lower()}': len(fold_df[fold_df[stratify_by] == cls]) for cls in cls_list\n",
    "            }\n",
    "        })\n",
    "    return pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    @property\n",
    "    def backbone(self) -> torch.nn.Module:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def backbone_named_parameters(self) -> t.Iterator[t.Tuple[str, torch.nn.parameter.Parameter]]:\n",
    "        return self.backbone.named_parameters()\n",
    "\n",
    "    @property\n",
    "    def head_named_parameters(self) -> t.Iterator[t.Tuple[str, torch.nn.parameter.Parameter]]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def resize_token_embeddings(self, num_tokens: int):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, **inputs: ModelInputValue) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def load_backbone(self, from_checkpoint: str):\n",
    "        self.backbone.load_state_dict(\n",
    "            torch.load(from_checkpoint, map_location=self.backbone.device))\n",
    "\n",
    "\n",
    "class ModelBuilder:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone_checkpoint: str,\n",
    "            num_classes: int,\n",
    "            enable_gradient_checkpointing: bool = False):\n",
    "        self._backbone_checkpoint = backbone_checkpoint\n",
    "        self._num_classes = num_classes\n",
    "        self._enable_gradient_checkpointing = enable_gradient_checkpointing\n",
    "\n",
    "    def build(self) -> Model:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ClsTokenPooler(torch.nn.Module):\n",
    "\n",
    "    def forward(self, features: torch.Tensor, mask: torch.Tensor | None) -> torch.Tensor:\n",
    "        return features[:, 0, :]\n",
    "\n",
    "\n",
    "class AttentionHeadPooler(torch.nn.Module):\n",
    "    def __init__(self, h_size: int, hidden_dim: int | None = None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim if hidden_dim is not None else h_size\n",
    "        self._attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_size, hidden_dim),\n",
    "            torch.nn.LayerNorm(hidden_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(hidden_dim, 1))\n",
    "\n",
    "    def forward(self, features: torch.Tensor, mask: torch.Tensor | None) -> torch.Tensor:\n",
    "        score = self._attention(features)\n",
    "        if mask is not None:\n",
    "            score[mask == 0] = float('-inf')\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class MultiStagedDropout(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            classifier: torch.nn.Module,\n",
    "            num_stages: int,\n",
    "            start_prob: float,\n",
    "            increment: float,\n",
    "            dropout_cls: t.Type[torch.nn.Module] = StableDropout):\n",
    "        super().__init__()\n",
    "        self._classifier = classifier\n",
    "        self._dropout_list = torch.nn.ModuleList([\n",
    "            dropout_cls(start_prob + (increment * i)) for i in range(num_stages)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.stack([self._classifier(drop(x)) for drop in self._dropout_list], dim=0).mean(dim=0)\n",
    "\n",
    "\n",
    "class _AutoModel(Model):\n",
    "\n",
    "    def __init__(self, backbone_checkpoint: str, num_classes: int):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(backbone_checkpoint)\n",
    "        config.output_hidden_states = True\n",
    "        print(f'Original hidden dropout: {config.hidden_dropout_prob}')\n",
    "        self._transformer = AutoModel.from_pretrained(backbone_checkpoint, config=config)\n",
    "        self._pooler = AttentionHeadPooler(h_size=config.hidden_size)\n",
    "        # self._pooler = ClsTokenPooler()\n",
    "        self._classifier = torch.nn.Sequential(\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            # torch.nn.LayerNorm(config.hidden_size),\n",
    "            MultiStagedDropout(\n",
    "                classifier=torch.nn.Linear(in_features=config.hidden_size, out_features=num_classes),\n",
    "                num_stages=5,\n",
    "                # start_prob=config.hidden_dropout_prob - 0.02,\n",
    "                start_prob=0.1,\n",
    "                increment=0.1),\n",
    "            # torch.nn.Linear(in_features=config.hidden_size, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def backbone(self) -> torch.nn.Module:\n",
    "        return self._transformer\n",
    "\n",
    "    @property\n",
    "    def head_named_parameters(self) -> t.Iterator[t.Tuple[str, torch.nn.parameter.Parameter]]:\n",
    "        return itertools.chain(\n",
    "            self._pooler.named_parameters(),\n",
    "            self._classifier.named_parameters())\n",
    "\n",
    "    def resize_token_embeddings(self, num_tokens: int):\n",
    "        self._transformer.resize_token_embeddings(num_tokens)\n",
    "\n",
    "    def forward(self, **inputs: ModelInputValue) -> torch.Tensor:\n",
    "        transformer_outputs = self._transformer(**inputs)\n",
    "        x = transformer_outputs.hidden_states[-1]\n",
    "        x = self._pooler(x, mask=inputs['attention_mask'])\n",
    "        return self._classifier(x)\n",
    "\n",
    "\n",
    "class AutoModelBuilder(ModelBuilder):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone_checkpoint: str,\n",
    "            num_classes: int,\n",
    "            enable_gradient_checkpointing: bool = False,\n",
    "            pretrained_backbone_checkpoint: str | None = None):\n",
    "        super().__init__(\n",
    "            backbone_checkpoint=backbone_checkpoint,\n",
    "            num_classes=num_classes,\n",
    "            enable_gradient_checkpointing=enable_gradient_checkpointing)\n",
    "        self._pretrained_backbone_checkpoint = pretrained_backbone_checkpoint\n",
    "\n",
    "    def build(self) -> Model:\n",
    "        model = _AutoModel(self._backbone_checkpoint, num_classes=self._num_classes)\n",
    "        if self._enable_gradient_checkpointing:\n",
    "            model.backbone.gradient_checkpointing_enable()  # type: ignore\n",
    "        if self._pretrained_backbone_checkpoint is not None:\n",
    "            model.load_backbone(self._pretrained_backbone_checkpoint)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_T = t.TypeVar('_T')\n",
    "\n",
    "Pred1D = t.List[np.ndarray]\n",
    "Pred2D = t.List[t.List[np.ndarray]]\n",
    "Ensemble1DStrategy = t.Callable[[Pred1D], np.ndarray]\n",
    "Ensemble2DStrategy = t.Callable[[Pred2D], np.ndarray]\n",
    "\n",
    "\n",
    "def _np_mean(x_list: t.List[np.ndarray]) -> np.ndarray:\n",
    "    return np.stack(x_list, axis=0).mean(axis=0)\n",
    "\n",
    "\n",
    "def _softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    return np.exp(x) / np.exp(x).sum(axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def mean_1d_ensemble_strategy(pred_1d: Pred1D) -> np.ndarray:\n",
    "    return _softmax(_np_mean(pred_1d), axis=-1)\n",
    "\n",
    "\n",
    "def mean_2d_ensemble_strategy(pred_2d: Pred2D) -> np.ndarray:\n",
    "    return _softmax(_np_mean([_np_mean(x_list) for x_list in pred_2d]), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def do_pred_iteration(\n",
    "        model: Model,\n",
    "        data_loader: DataLoader,\n",
    "        device: str,) -> np.ndarray:\n",
    "    model.eval()\n",
    "    score_list = []\n",
    "\n",
    "    it = tqdm(enumerate(data_loader), desc='Validating.', total=len(data_loader))\n",
    "    batch: DatasetItem\n",
    "    for step, batch in it:\n",
    "        model_input = batch.tokenizer_result.get_model_input(device=device)\n",
    "\n",
    "        logit = model(**model_input).squeeze(-1)\n",
    "        pred = torch_f.softmax(logit, dim=-1)\n",
    "\n",
    "        score_list.append([[elem.item() for elem in x] for x in pred.cpu()])\n",
    "\n",
    "    return np.array(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_gpu_device(device: t.Union[str, torch.device]) -> bool:\n",
    "    return str(device).startswith('cuda')\n",
    "\n",
    "def _predict_by_model(\n",
    "        df: pd.DataFrame,\n",
    "        model_builder: ModelBuilder,\n",
    "        tokenizer: Tokenizer,\n",
    "        device: str,\n",
    "        batch_size: int,\n",
    "        max_len: int,\n",
    "        num_workers: int,\n",
    "        collator: Collator | None = None,\n",
    "        add_new_special_tokens: bool = False,\n",
    "        ) -> np.ndarray:\n",
    "    dataset = Dataset(\n",
    "        df=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len)\n",
    "    dataset.sort_by_tokenizer_input_len()\n",
    "\n",
    "    collator = collator if collator is not None else Collator(default_collate_fn)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=collator,  # type: ignore\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=_is_gpu_device(device))\n",
    "    model = model_builder.build().to(device)\n",
    "\n",
    "    if add_new_special_tokens:\n",
    "        special_token_list = parse_special_token_list(df)\n",
    "        print(f'Adding new special tokens: {special_token_list}')\n",
    "        tokenizer.add_special_token_list(special_token_list)\n",
    "        if tokenizer.num_special_tokens > 0:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return do_pred_iteration(\n",
    "        model=model,\n",
    "        data_loader=data_loader,\n",
    "        device=device)\n",
    "\n",
    "\n",
    "def _arr_to_score_dict(arr: np.ndarray) -> t.Dict[str, float]:\n",
    "    return {\n",
    "        'score_ineffective': arr[0],\n",
    "        'score_adequate': arr[1],\n",
    "        'score_effective': arr[2],\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_oof_by_k_fold_model(\n",
    "        all_df: pd.DataFrame,\n",
    "        fold_list: t.List[int],\n",
    "        model_builder_list: t.List[ModelBuilder],\n",
    "        tokenizer: Tokenizer,\n",
    "        device: str,\n",
    "        batch_size: int,\n",
    "        max_len: int,\n",
    "        num_workers: int,\n",
    "        collator: Collator | None = None,\n",
    "        add_new_special_tokens: bool = False) -> pd.DataFrame:\n",
    "    id_list: t.List[str] = []\n",
    "    score_arr_list: t.List[np.ndarray] = []\n",
    "    for fold, model_builder in zip(fold_list, model_builder_list):\n",
    "        df = dataframe(all_df[all_df['fold'] == fold])\n",
    "        id_list.extend(df['id'].to_list())\n",
    "        score_arr = _predict_by_model(\n",
    "            df=df,\n",
    "            model_builder=model_builder,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            max_len=max_len,\n",
    "            num_workers=num_workers,\n",
    "            collator=collator,\n",
    "            add_new_special_tokens=add_new_special_tokens)\n",
    "        score_arr_list.append(score_arr)\n",
    "    return pd.DataFrame({\n",
    "        'id': id_list,\n",
    "        'score': np.concatenate(score_arr_list, axis=0),\n",
    "    })\n",
    "\n",
    "\n",
    "def predict_by_k_fold_model_list(\n",
    "        df: pd.DataFrame,\n",
    "        model_builder_2dlist: t.List[t.List[ModelBuilder]],\n",
    "        ensemble_strategy: Ensemble2DStrategy,\n",
    "        tokenizer: Tokenizer,\n",
    "        device: str,\n",
    "        batch_size: int,\n",
    "        max_len: int,\n",
    "        num_workers: int,\n",
    "        collator: Collator | None = None,\n",
    "        add_new_special_tokens: bool = False) -> pd.DataFrame:\n",
    "    id_list = df['id'].to_list()\n",
    "    pred_2d: t.List[t.List[np.ndarray]] = []\n",
    "    for model_builder_list in model_builder_2dlist:\n",
    "        pred_1d: t.List[np.ndarray] = []\n",
    "        for model_builder in model_builder_list:\n",
    "            pred_1d.append(_predict_by_model(\n",
    "                df=df,\n",
    "                model_builder=model_builder,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device,\n",
    "                batch_size=batch_size,\n",
    "                max_len=max_len,\n",
    "                num_workers=num_workers,\n",
    "                collator=collator,\n",
    "                add_new_special_tokens=add_new_special_tokens))\n",
    "        pred_2d.append(pred_1d)\n",
    "    score_arr = ensemble_strategy(id_list)\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'id': id,\n",
    "            **_arr_to_score_dict(arr)\n",
    "        }\n",
    "        for id, arr in zip(id_list, score_arr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DEVICE = 'cuda'\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "CONTEST_NAME = 'feedback-prize-effectiveness'\n",
    "\n",
    "GDRIVE_DIR = Path('/content/drive/MyDrive')\n",
    "GDRIVE_DATA_DIR = GDRIVE_DIR / 'Data'\n",
    "DATASET_GDRIVE_DIR = GDRIVE_DATA_DIR / CONTEST_NAME\n",
    "\n",
    "MODELS_DIR = GDRIVE_DIR / f'models/{CONTEST_NAME}'\n",
    "OUTPUT_DIR = GDRIVE_DIR / f'oof/{CONTEST_NAME}/seq-cls'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "MAX_LEN = 512\n",
    "NUM_WORKERS = 2\n",
    "BACKBONE = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "all_df = read_csv(DATASET_GDRIVE_DIR / 'train_ext.csv')\n",
    "all_df = StratifyAndGroupByColumnValStrategy(\n",
    "    stratify_by='discourse_effectiveness',\n",
    "    group_by='essay_id',\n",
    "    seed=SEED\n",
    ").assign_folds(all_df, num_folds=NUM_FOLDS)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = get_tokenizer_for_backbone(BACKBONE, padding_strategy=PaddingStrategy.DO_NOT_PAD)\n",
    "\n",
    "oof_df = predict_oof_by_k_fold_model(\n",
    "    all_df=all_df,\n",
    "    model_builder_list=[\n",
    "        AutoModelBuilder(\n",
    "            backbone_checkpoint=BACKBONE,\n",
    "            num_classes=3,\n",
    "            pretrained_backbone_checkpoint=str(MODELS_DIR / f'microsoft-deberta-v3-large-v2-1-inf-fold_{fold}-seed_42-gpu_teslap100-pcie-16gb.pt'))\n",
    "        for fold in range(NUM_FOLDS)\n",
    "    ],\n",
    "    tokenizer=_tokenizer,\n",
    "    fold_list=list(range(NUM_FOLDS)),\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collator=Collator(DataCollatorWithPadding(_tokenizer.tokenizer)),\n",
    "    add_new_special_tokens=True)\n",
    "oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df.to_csv(OUTPUT_DIR / 'microsoft-deberta-v3-large-v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
